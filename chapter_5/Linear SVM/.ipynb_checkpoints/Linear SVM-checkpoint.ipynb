{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c164f239",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "<img src=\"1.png\">\n",
    "\n",
    "if we looked at the image on the left first the dashed green line is so bad because it underfitting the data, and the other two line not good because it seprate the data but not generalize for new instance, but on the other hand look at the model on the right you will see large space btween the different classes, which generalize well for new instance, this idea is what we called **SVM**, the two dashed line called **support Vecotrs**, they seprate the different classes, then our decsion boundary line in not just seprate the data but also stays as far away from the closest training instances as possible.\n",
    "\n",
    "# Soft Margin Classification\n",
    "\n",
    "<img src=\"2.png\">\n",
    "\n",
    "We can stricly push the earlier as we see from left to right graph the yello instanse is moved, this is stricly impossed is called **Hard Margin Classification**, and this work just in linearly separable data, and it sensitive to the outlier, and this is not good because these outlier will effect the large margin to be fewer margin.\n",
    "\n",
    "To avoid this we should accept that we can have an instance in the wrong side or in the margin space as in the image below, so we give the flexibility to the model to generalize well as in the left graph not to be sensitive as in the right graph.\n",
    "\n",
    "<img src=\"3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d22ac8",
   "metadata": {},
   "source": [
    "# SVM With sklearn\n",
    "\n",
    "We can control this margin using the **C** hyperparameter as if it small will generalize well while large number for it will affect to problem of **hard margin** and the model will be sensitive to outliers. Also we can noticed from generalization is that **C** can be reduced to reduce the overfitting.\n",
    "\n",
    "Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639f99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327edf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica Binary classifcation\n",
    "\n",
    "# for scaling and then train the model with C=1, hinge is function will talked about later\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86abcb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica Binary classifcation\n",
    "\n",
    "# for scaling and then train the model with C=1, hinge is function will talked about later\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", SVC(kernel=\"linear\",C=1)),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9e9a6",
   "metadata": {},
   "source": [
    "# Note !\n",
    "The model below applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it can  be  useful  to  handle  huge  datasets  that  do  not  fit  in  memory  (out-of-core  training), or to handle online classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e7fcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica Binary classifcation\n",
    "\n",
    "# \n",
    "sgd_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", SGDClassifier(loss=\"hinge\",alpha=1/(len(y)*1))),\n",
    "    ])\n",
    "\n",
    "sgd_svm_clf.fit(X, y)\n",
    "\n",
    "sgd_svm_clf.predict([[5.5, 1.7]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
