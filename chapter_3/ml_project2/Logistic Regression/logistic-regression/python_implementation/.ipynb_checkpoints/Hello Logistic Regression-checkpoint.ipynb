{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abdelrahman Rezk\n",
    "# ML & NLP Student\n",
    "\n",
    "# Web Developer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Binary Classification with multiple of features\"\n",
    "This is a Logistic Regression (Classification) example that explain at basic level how the model work \n",
    "with simple data that will create every thing from 0 level and steps of work will be on these function\n",
    "h(x) predicted value\n",
    "cost function\n",
    "gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "(5, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Ahmed', '1', '18', '70'],\n",
       "       ['Hamed', '0', '22', '88'],\n",
       "       ['Mona', '0', '38', '91'],\n",
       "       ['Said', '1', '21', '65'],\n",
       "       ['Lobna', '1', '25', '79']], dtype='<U5')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [\"Ahmed\", 1, 18, 70],\n",
    "    [\"Hamed\",0, 22 ,88],\n",
    "    [\"Mona\",0, 38, 91],\n",
    "    [\"Said\",1, 21, 65],\n",
    "    [\"Lobna\",1, 25, 79]\n",
    "])\n",
    "print(\"###########################################\")\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "(5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0]\n",
    "    \n",
    "])\n",
    "print(\"###########################################\")\n",
    "\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first \n",
    "    we should remove the feature name \n",
    "    that is will not affect our algorithm with nothing\n",
    "    actually maybe lead to misleading so we don't care of using the name feature\n",
    "# second\n",
    "    we will doing x0=1 for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7804267 , -0.7804267 , -0.2549909 ,  1.35222448],\n",
       "       [-0.7804267 , -0.81133469, -0.13135895,  1.90856826],\n",
       "       [-0.7804267 , -0.81133469,  0.36316886,  2.00129223],\n",
       "       [-0.7804267 , -0.7804267 , -0.16226694,  1.19768454],\n",
       "       [-0.7804267 , -0.7804267 , -0.03863499,  1.63039637]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete names feature\n",
    "X = np.delete(X, 0, axis=1)\n",
    "X = X.astype(int)\n",
    "# add x0 = 1\n",
    "X = np.column_stack([np.ones((5,1), dtype=int), X])\n",
    "# features normalization\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "(4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    assume that values at beginning\n",
    "    there are a four theta for 4 features\n",
    "    actually we don't need features normalization here\n",
    "    because our cost function and gradient descent using h(x) as sigmoid function \n",
    "    which help us to get values between 0 and 1 but\n",
    "    we need to handle decision boundary using the idea of probability\n",
    "    that if h(x) >= .5 we will assign 1 for this examples and h(x) < .5 will assign 0\n",
    "    \n",
    "'''\n",
    "\n",
    "thetas = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]\n",
    "])\n",
    "print(\"###########################################\")\n",
    "print(thetas.shape)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "    implemented using vectorized version\n",
    "Now we should calculate the cost function that help us knowing what the cost error we have from prediction values and actual values.\n",
    "\n",
    "\n",
    "<font color='green'> J(thetas) = (-1/m) * transpose(y)log(h(x)) + transpose(1-y)log(1-h(x)) </font>\n",
    "\n",
    "<font color=\"green\"> h(x) = g(z) & z = X*thetas</font>\n",
    "\n",
    "<font color='green'> h(x) = g(z) = 1/(1 + exp(-z)) </font>\n",
    "\n",
    "sometime we will handle that to be theta transpose and X because of will vectorize everything without loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfunction(x, theats, y,m):\n",
    "    '''\n",
    "        this function will return the cost function as we explain above\n",
    "        arguments\n",
    "            x is the features of our training examples\n",
    "            thetas that what we need to get at the end \n",
    "            which minimize our cost function with new examples            \n",
    "            y is the actual value for each training example\n",
    "            m the number of our training examples\n",
    "        returned value\n",
    "            the cost for the all training examples in J\n",
    "    '''\n",
    "    '''\n",
    "    unvectorize version\n",
    "    J(thetas) = (-1/m) * sum(ylog(h(x)) + (1-y)log(1-h(x)))\n",
    "        let us implement each step for understanding\n",
    "        h_x = np.dot(x, theats)\n",
    "        below we get the difference between actual and predicted values\n",
    "        cost_error = np.sum(y*log(h_x)+(1-y)*log(1-h_x))\n",
    "        cost_error = (-1/m) * cost_error\n",
    "    '''\n",
    "\n",
    "    J = 0\n",
    "    h_x = 1 / (1+ (np.exp(-(np.dot(X, thetas)))))\n",
    "    part_1 = np.dot(y.T, np.log(h_x))\n",
    "    part_2 = np.dot((1-y).T, (1 - np.log(h_x)))\n",
    "    \n",
    "    J = (-1/m) *  (part_1 + part_2)\n",
    "    print(\"#############################################\")\n",
    "    print(\"divide cost_erro by 2*m \\n\",h_x)\n",
    "    return abs(J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.38612745]\n",
      " [0.54622957]\n",
      " [0.6841046 ]\n",
      " [0.37158204]\n",
      " [0.50772638]]\n",
      "#####################################\n",
      "cost error is \n",
      " [[0.34637001]]\n"
     ]
    }
   ],
   "source": [
    "cost_error = costfunction(X, thetas, y, X.shape[0])\n",
    "print(\"#####################################\")\n",
    "print(\"cost error is \\n\", cost_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "Now we should calculate thetas and reassigned it \n",
    "for new values with each iteration of gradient descent \n",
    "because we need the values of theta that minimize our cost function\n",
    "we calculate above with each step of gradient decent we will calculate cost function to check if its actually minimized or not\n",
    "\n",
    "vectorized version \n",
    "\n",
    "<font color='red'> thetas = thetas - (alpha/m) * transpose(X)*(g(X*thetas)-y) </font>\n",
    "\n",
    "<font color='green'> h(x) = g(X*thetas) = 1/(1 + exp(-(X*thetas)) </font>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent(x,thetas,y,m,alpha):\n",
    "#     predicted_actual = np.power(h_x-y,2)\n",
    "#     mutiply_by_x = predicted_actual * x # here is a element wise multiplication\n",
    "    '''\n",
    "        we use vectorize version \n",
    "    '''\n",
    "    '''\n",
    "        unvectorized version\n",
    "        thetas = thetas -(alpha/m) * sumation(((h(x) - y))*x)\n",
    "    '''\n",
    "    h_x = 1 / (1+ (np.exp(-(np.dot(X, thetas)))))\n",
    "    thetas = thetas - (alpha/m) * np.dot(X.T, (h_x-y)) \n",
    "    \n",
    "    return thetas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99291674],\n",
       "       [0.99248854],\n",
       "       [0.99961293],\n",
       "       [1.0189992 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = gradientdescent(X,thetas,y,X.shape[0],.09)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.40314357]\n",
      " [0.56889555]\n",
      " [0.70424933]\n",
      " [0.38700691]\n",
      " [0.52800544]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98653462]\n",
      " [0.98569071]\n",
      " [0.99925601]\n",
      " [1.03651032]]\n",
      "cost after\n",
      " [[0.35296353]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.41067374]\n",
      " [0.57876008]\n",
      " [0.71285492]\n",
      " [0.3938315 ]\n",
      " [0.53687405]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98079334]\n",
      " [0.97954505]\n",
      " [0.9989281 ]\n",
      " [1.05266333]]\n",
      "cost after\n",
      " [[0.35570461]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.41761219]\n",
      " [0.58776035]\n",
      " [0.72062293]\n",
      " [0.40011887]\n",
      " [0.54498938]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97563776]\n",
      " [0.97399537]\n",
      " [0.99862793]\n",
      " [1.06757677]]\n",
      "cost after\n",
      " [[0.35814106]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4240021 ]\n",
      " [0.59597463]\n",
      " [0.72764406]\n",
      " [0.40590807]\n",
      " [0.55241615]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97101753]\n",
      " [0.96899036]\n",
      " [0.99835411]\n",
      " [1.08135859]]\n",
      "cost after\n",
      " [[0.36031297]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.42988455]\n",
      " [0.60347485]\n",
      " [0.73399861]\n",
      " [0.41123624]\n",
      " [0.55921421]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96688674]\n",
      " [0.96448327]\n",
      " [0.99810522]\n",
      " [1.094107  ]]\n",
      "cost after\n",
      " [[0.36225449]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.43529829]\n",
      " [0.61032668]\n",
      " [0.73975766]\n",
      " [0.41613837]\n",
      " [0.56543862]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96320356]\n",
      " [0.9604315 ]\n",
      " [0.99787981]\n",
      " [1.10591129]]\n",
      "cost after\n",
      " [[0.36399474]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.44027964]\n",
      " [0.61658987]\n",
      " [0.74498402]\n",
      " [0.42064722]\n",
      " [0.57113979]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95992988]\n",
      " [0.95679624]\n",
      " [0.99767645]\n",
      " [1.11685261]]\n",
      "cost after\n",
      " [[0.36555862]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.44486245]\n",
      " [0.62231863]\n",
      " [0.74973324]\n",
      " [0.42479337]\n",
      " [0.57636378]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95703101]\n",
      " [0.95354219]\n",
      " [0.99749373]\n",
      " [1.12700473]]\n",
      "cost after\n",
      " [[0.36696753]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.44907817]\n",
      " [0.62756203]\n",
      " [0.75405444]\n",
      " [0.42860522]\n",
      " [0.58115256]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95447534]\n",
      " [0.95063716]\n",
      " [0.99733029]\n",
      " [1.13643472]]\n",
      "cost after\n",
      " [[0.36823984]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.45295591]\n",
      " [0.63236446]\n",
      " [0.75799112]\n",
      " [0.43210907]\n",
      " [0.58554428]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95223406]\n",
      " [0.94805185]\n",
      " [0.99718481]\n",
      " [1.14520359]]\n",
      "cost after\n",
      " [[0.36939145]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.45652259]\n",
      " [0.636766  ]\n",
      " [0.76158186]\n",
      " [0.43532924]\n",
      " [0.58957368]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95028094]\n",
      " [0.94575956]\n",
      " [0.99705604]\n",
      " [1.15336685]]\n",
      "cost after\n",
      " [[0.3704361]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.45980301]\n",
      " [0.64080288]\n",
      " [0.7648609 ]\n",
      " [0.4382882 ]\n",
      " [0.59327226]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94859204]\n",
      " [0.94373593]\n",
      " [0.99694279]\n",
      " [1.16097507]]\n",
      "cost after\n",
      " [[0.37138573]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.46282004]\n",
      " [0.64450783]\n",
      " [0.76785866]\n",
      " [0.44100664]\n",
      " [0.59666869]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94714551]\n",
      " [0.94195874]\n",
      " [0.99684391]\n",
      " [1.16807431]]\n",
      "cost after\n",
      " [[0.37225075]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.46559471]\n",
      " [0.64791041]\n",
      " [0.77060225]\n",
      " [0.44350363]\n",
      " [0.59978898]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94592142]\n",
      " [0.94040773]\n",
      " [0.99675835]\n",
      " [1.17470656]]\n",
      "cost after\n",
      " [[0.37304023]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.46814637]\n",
      " [0.65103737]\n",
      " [0.77311585]\n",
      " [0.44579671]\n",
      " [0.60265678]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94490156]\n",
      " [0.93906436]\n",
      " [0.99668509]\n",
      " [1.18091014]]\n",
      "cost after\n",
      " [[0.37376216]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47049279]\n",
      " [0.6539129 ]\n",
      " [0.77542107]\n",
      " [0.44790204]\n",
      " [0.60529358]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94406928]\n",
      " [0.93791171]\n",
      " [0.9966232 ]\n",
      " [1.18672005]]\n",
      "cost after\n",
      " [[0.37442351]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47265032]\n",
      " [0.65655889]\n",
      " [0.77753723]\n",
      " [0.44983446]\n",
      " [0.60771892]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94340935]\n",
      " [0.9369343 ]\n",
      " [0.9965718 ]\n",
      " [1.19216825]]\n",
      "cost after\n",
      " [[0.37503047]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47463397]\n",
      " [0.65899518]\n",
      " [0.77948167]\n",
      " [0.45160764]\n",
      " [0.60995061]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94290784]\n",
      " [0.93611795]\n",
      " [0.99653005]\n",
      " [1.19728397]]\n",
      "cost after\n",
      " [[0.37558846]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47645753]\n",
      " [0.66123978]\n",
      " [0.78126996]\n",
      " [0.45323412]\n",
      " [0.61200482]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.942552  ]\n",
      " [0.93544971]\n",
      " [0.99649718]\n",
      " [1.20209394]]\n",
      "cost after\n",
      " [[0.37610232]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47813369]\n",
      " [0.663309  ]\n",
      " [0.78291609]\n",
      " [0.45472545]\n",
      " [0.61389633]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94233014]\n",
      " [0.93491769]\n",
      " [0.99647249]\n",
      " [1.20662264]]\n",
      "cost after\n",
      " [[0.37657633]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.47967408]\n",
      " [0.66521769]\n",
      " [0.78443267]\n",
      " [0.45609227]\n",
      " [0.61563857]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94223153]\n",
      " [0.93451099]\n",
      " [0.99645529]\n",
      " [1.2108925 ]]\n",
      "cost after\n",
      " [[0.3770143]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4810894 ]\n",
      " [0.66697934]\n",
      " [0.78583109]\n",
      " [0.45734432]\n",
      " [0.61724384]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94224636]\n",
      " [0.93421964]\n",
      " [0.99644496]\n",
      " [1.21492407]]\n",
      "cost after\n",
      " [[0.37741962]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4823895 ]\n",
      " [0.66860623]\n",
      " [0.78712162]\n",
      " [0.45849061]\n",
      " [0.61872332]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94236559]\n",
      " [0.93403445]\n",
      " [0.99644092]\n",
      " [1.21873619]]\n",
      "cost after\n",
      " [[0.37779533]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48358342]\n",
      " [0.67010953]\n",
      " [0.78831357]\n",
      " [0.45953938]\n",
      " [0.62008727]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94258096]\n",
      " [0.93394701]\n",
      " [0.99644264]\n",
      " [1.22234616]]\n",
      "cost after\n",
      " [[0.37814416]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48467947]\n",
      " [0.67149944]\n",
      " [0.78941535]\n",
      " [0.46049825]\n",
      " [0.62134504]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94288486]\n",
      " [0.93394961]\n",
      " [0.99644961]\n",
      " [1.22576985]]\n",
      "cost after\n",
      " [[0.37846853]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4856853 ]\n",
      " [0.67278526]\n",
      " [0.79043462]\n",
      " [0.46137422]\n",
      " [0.6225052 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94327029]\n",
      " [0.93403512]\n",
      " [0.99646138]\n",
      " [1.22902183]]\n",
      "cost after\n",
      " [[0.37877066]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48660796]\n",
      " [0.67397548]\n",
      " [0.79137831]\n",
      " [0.46217374]\n",
      " [0.62357559]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94373084]\n",
      " [0.93419704]\n",
      " [0.9964775 ]\n",
      " [1.23211551]]\n",
      "cost after\n",
      " [[0.3790525]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48745391]\n",
      " [0.67507786]\n",
      " [0.79225273]\n",
      " [0.46290272]\n",
      " [0.62456337]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94426059]\n",
      " [0.93442934]\n",
      " [0.99649758]\n",
      " [1.2350632 ]]\n",
      "cost after\n",
      " [[0.37931584]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48822912]\n",
      " [0.6760995 ]\n",
      " [0.79306364]\n",
      " [0.46356665]\n",
      " [0.62547514]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94485411]\n",
      " [0.93472652]\n",
      " [0.99652126]\n",
      " [1.23787623]]\n",
      "cost after\n",
      " [[0.37956229]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48893908]\n",
      " [0.67704689]\n",
      " [0.79381625]\n",
      " [0.46417055]\n",
      " [0.62631693]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.9455064 ]\n",
      " [0.93508348]\n",
      " [0.99654817]\n",
      " [1.240565  ]]\n",
      "cost after\n",
      " [[0.37979331]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.48958884]\n",
      " [0.67792597]\n",
      " [0.79451535]\n",
      " [0.46471907]\n",
      " [0.62709427]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94621286]\n",
      " [0.93549555]\n",
      " [0.99657802]\n",
      " [1.24313912]]\n",
      "cost after\n",
      " [[0.38001022]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49018306]\n",
      " [0.67874219]\n",
      " [0.7951653 ]\n",
      " [0.46521647]\n",
      " [0.62781225]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94696923]\n",
      " [0.93595842]\n",
      " [0.9966105 ]\n",
      " [1.2456074 ]]\n",
      "cost after\n",
      " [[0.38021421]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49072602]\n",
      " [0.67950055]\n",
      " [0.79577006]\n",
      " [0.46566669]\n",
      " [0.62847553]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94777163]\n",
      " [0.93646813]\n",
      " [0.99664534]\n",
      " [1.24797796]]\n",
      "cost after\n",
      " [[0.38040636]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49122169]\n",
      " [0.68020563]\n",
      " [0.79633329]\n",
      " [0.46607337]\n",
      " [0.62908839]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94861644]\n",
      " [0.93702101]\n",
      " [0.99668229]\n",
      " [1.2502583 ]]\n",
      "cost after\n",
      " [[0.38058767]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4916737 ]\n",
      " [0.68086162]\n",
      " [0.79685832]\n",
      " [0.46643986]\n",
      " [0.62965478]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.94950035]\n",
      " [0.9376137 ]\n",
      " [0.99672111]\n",
      " [1.25245531]]\n",
      "cost after\n",
      " [[0.38075904]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49208542]\n",
      " [0.68147239]\n",
      " [0.79734818]\n",
      " [0.46676923]\n",
      " [0.63017832]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95042031]\n",
      " [0.93824309]\n",
      " [0.99676159]\n",
      " [1.25457535]]\n",
      "cost after\n",
      " [[0.38092128]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49245993]\n",
      " [0.68204149]\n",
      " [0.79780568]\n",
      " [0.46706435]\n",
      " [0.63066234]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.9513735 ]\n",
      " [0.93890632]\n",
      " [0.99680353]\n",
      " [1.25662427]]\n",
      "cost after\n",
      " [[0.38107514]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4928001 ]\n",
      " [0.68257217]\n",
      " [0.79823338]\n",
      " [0.46732784]\n",
      " [0.63110992]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95235731]\n",
      " [0.93960075]\n",
      " [0.99684674]\n",
      " [1.25860748]]\n",
      "cost after\n",
      " [[0.38122131]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49310856]\n",
      " [0.68306744]\n",
      " [0.79863362]\n",
      " [0.46756212]\n",
      " [0.63152387]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95336935]\n",
      " [0.94032394]\n",
      " [0.99689106]\n",
      " [1.26052995]]\n",
      "cost after\n",
      " [[0.3813604]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49338775]\n",
      " [0.68353004]\n",
      " [0.79900857]\n",
      " [0.46776942]\n",
      " [0.63190678]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95440741]\n",
      " [0.94107365]\n",
      " [0.99693631]\n",
      " [1.26239629]]\n",
      "cost after\n",
      " [[0.381493]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4936399 ]\n",
      " [0.68396251]\n",
      " [0.7993602 ]\n",
      " [0.46795183]\n",
      " [0.63226107]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95546945]\n",
      " [0.94184781]\n",
      " [0.99698236]\n",
      " [1.26421072]]\n",
      "cost after\n",
      " [[0.3816196]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49386708]\n",
      " [0.68436718]\n",
      " [0.79969033]\n",
      " [0.46811123]\n",
      " [0.63258892]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95655359]\n",
      " [0.94264449]\n",
      " [0.99702907]\n",
      " [1.26597717]]\n",
      "cost after\n",
      " [[0.38174071]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4940712 ]\n",
      " [0.68474621]\n",
      " [0.80000064]\n",
      " [0.46824938]\n",
      " [0.63289238]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95765809]\n",
      " [0.94346195]\n",
      " [0.99707632]\n",
      " [1.26769923]]\n",
      "cost after\n",
      " [[0.38185675]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49425404]\n",
      " [0.68510157]\n",
      " [0.80029266]\n",
      " [0.46836792]\n",
      " [0.63317332]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95878134]\n",
      " [0.94429854]\n",
      " [0.99712398]\n",
      " [1.26938024]]\n",
      "cost after\n",
      " [[0.38196812]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49441721]\n",
      " [0.68543508]\n",
      " [0.8005678 ]\n",
      " [0.46846834]\n",
      " [0.63343346]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.95992186]\n",
      " [0.94515276]\n",
      " [0.99717196]\n",
      " [1.27102327]]\n",
      "cost after\n",
      " [[0.38207519]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49456221]\n",
      " [0.68574843]\n",
      " [0.80082738]\n",
      " [0.46855203]\n",
      " [0.6336744 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96107829]\n",
      " [0.94602324]\n",
      " [0.99722015]\n",
      " [1.27263116]]\n",
      "cost after\n",
      " [[0.38217831]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49469044]\n",
      " [0.68604317]\n",
      " [0.80107258]\n",
      " [0.46862026]\n",
      " [0.6338976 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96224937]\n",
      " [0.94690867]\n",
      " [0.99726846]\n",
      " [1.27420653]]\n",
      "cost after\n",
      " [[0.38227777]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49480318]\n",
      " [0.68632071]\n",
      " [0.80130451]\n",
      " [0.46867421]\n",
      " [0.63410442]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96343393]\n",
      " [0.94780789]\n",
      " [0.99731681]\n",
      " [1.2757518 ]]\n",
      "cost after\n",
      " [[0.38237386]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4949016 ]\n",
      " [0.68658238]\n",
      " [0.80152419]\n",
      " [0.46871498]\n",
      " [0.63429611]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96463089]\n",
      " [0.9487198 ]\n",
      " [0.99736512]\n",
      " [1.2772692 ]]\n",
      "cost after\n",
      " [[0.38246685]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4949868 ]\n",
      " [0.68682938]\n",
      " [0.80173254]\n",
      " [0.46874357]\n",
      " [0.63447382]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96583926]\n",
      " [0.94964338]\n",
      " [0.99741332]\n",
      " [1.2787608 ]]\n",
      "cost after\n",
      " [[0.38255698]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4950598 ]\n",
      " [0.68706285]\n",
      " [0.80193044]\n",
      " [0.46876092]\n",
      " [0.63463862]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96705812]\n",
      " [0.95057771]\n",
      " [0.99746133]\n",
      " [1.2802285 ]]\n",
      "cost after\n",
      " [[0.38264446]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49512152]\n",
      " [0.6872838 ]\n",
      " [0.80211868]\n",
      " [0.46876787]\n",
      " [0.6347915 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96828663]\n",
      " [0.95152192]\n",
      " [0.99750911]\n",
      " [1.28167407]]\n",
      "cost after\n",
      " [[0.38272951]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49517282]\n",
      " [0.6874932 ]\n",
      " [0.80229799]\n",
      " [0.46876523]\n",
      " [0.63493335]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.96952399]\n",
      " [0.95247522]\n",
      " [0.99755658]\n",
      " [1.28309913]]\n",
      "cost after\n",
      " [[0.3828123]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49521451]\n",
      " [0.68769192]\n",
      " [0.80246905]\n",
      " [0.46875374]\n",
      " [0.63506501]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.9707695 ]\n",
      " [0.95343687]\n",
      " [0.99760371]\n",
      " [1.28450518]]\n",
      "cost after\n",
      " [[0.38289301]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49524731]\n",
      " [0.68788077]\n",
      " [0.80263247]\n",
      " [0.46873406]\n",
      " [0.63518727]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97202247]\n",
      " [0.95440619]\n",
      " [0.99765043]\n",
      " [1.28589361]]\n",
      "cost after\n",
      " [[0.3829718]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4952719 ]\n",
      " [0.68806051]\n",
      " [0.80278884]\n",
      " [0.46870683]\n",
      " [0.63530082]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97328229]\n",
      " [0.95538257]\n",
      " [0.99769671]\n",
      " [1.2872657 ]]\n",
      "cost after\n",
      " [[0.38304881]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49528892]\n",
      " [0.68823182]\n",
      " [0.80293869]\n",
      " [0.46867262]\n",
      " [0.63540634]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97454839]\n",
      " [0.95636541]\n",
      " [0.99774251]\n",
      " [1.28862264]]\n",
      "cost after\n",
      " [[0.38312418]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49529894]\n",
      " [0.68839535]\n",
      " [0.8030825 ]\n",
      " [0.46863196]\n",
      " [0.63550443]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97582025]\n",
      " [0.95735418]\n",
      " [0.99778779]\n",
      " [1.28996552]]\n",
      "cost after\n",
      " [[0.38319804]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4953025 ]\n",
      " [0.68855168]\n",
      " [0.80322073]\n",
      " [0.46858537]\n",
      " [0.63559565]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97709737]\n",
      " [0.95834838]\n",
      " [0.9978325 ]\n",
      " [1.29129536]]\n",
      "cost after\n",
      " [[0.38327049]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49530009]\n",
      " [0.68870136]\n",
      " [0.80335379]\n",
      " [0.46853328]\n",
      " [0.63568053]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.9783793 ]\n",
      " [0.95934757]\n",
      " [0.99787663]\n",
      " [1.29261309]]\n",
      "cost after\n",
      " [[0.38334165]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49529218]\n",
      " [0.68884488]\n",
      " [0.80348206]\n",
      " [0.46847612]\n",
      " [0.63575953]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.97966564]\n",
      " [0.96035131]\n",
      " [0.99792013]\n",
      " [1.29391958]]\n",
      "cost after\n",
      " [[0.3834116]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49527918]\n",
      " [0.68898272]\n",
      " [0.80360591]\n",
      " [0.46841429]\n",
      " [0.63583312]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98095598]\n",
      " [0.96135921]\n",
      " [0.99796299]\n",
      " [1.29521562]]\n",
      "cost after\n",
      " [[0.38348045]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49526148]\n",
      " [0.68911531]\n",
      " [0.80372567]\n",
      " [0.46834814]\n",
      " [0.63590169]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98224999]\n",
      " [0.96237092]\n",
      " [0.99800518]\n",
      " [1.29650195]]\n",
      "cost after\n",
      " [[0.38354826]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49523945]\n",
      " [0.68924303]\n",
      " [0.80384162]\n",
      " [0.468278  ]\n",
      " [0.63596563]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98354732]\n",
      " [0.9633861 ]\n",
      " [0.99804667]\n",
      " [1.29777926]]\n",
      "cost after\n",
      " [[0.38361513]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49521343]\n",
      " [0.68936626]\n",
      " [0.80395406]\n",
      " [0.46820419]\n",
      " [0.63602529]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98484768]\n",
      " [0.96440445]\n",
      " [0.99808744]\n",
      " [1.29904817]]\n",
      "cost after\n",
      " [[0.38368111]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49518371]\n",
      " [0.68948534]\n",
      " [0.80406323]\n",
      " [0.46812699]\n",
      " [0.63608098]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98615079]\n",
      " [0.96542567]\n",
      " [0.99812747]\n",
      " [1.30030928]]\n",
      "cost after\n",
      " [[0.38374628]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49515059]\n",
      " [0.68960057]\n",
      " [0.80416939]\n",
      " [0.46804667]\n",
      " [0.63613301]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98745639]\n",
      " [0.9664495 ]\n",
      " [0.99816675]\n",
      " [1.30156312]]\n",
      "cost after\n",
      " [[0.38381069]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49511433]\n",
      " [0.68971224]\n",
      " [0.80427275]\n",
      " [0.46796345]\n",
      " [0.63618166]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.98876423]\n",
      " [0.96747571]\n",
      " [0.99820526]\n",
      " [1.30281018]]\n",
      "cost after\n",
      " [[0.3838744]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49507517]\n",
      " [0.68982063]\n",
      " [0.80437351]\n",
      " [0.46787758]\n",
      " [0.63622717]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.9900741 ]\n",
      " [0.96850406]\n",
      " [0.99824298]\n",
      " [1.30405093]]\n",
      "cost after\n",
      " [[0.38393746]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49503334]\n",
      " [0.68992598]\n",
      " [0.80447187]\n",
      " [0.46778926]\n",
      " [0.63626979]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99138579]\n",
      " [0.96953434]\n",
      " [0.9982799 ]\n",
      " [1.3052858 ]]\n",
      "cost after\n",
      " [[0.38399991]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49498904]\n",
      " [0.69002851]\n",
      " [0.80456799]\n",
      " [0.46769869]\n",
      " [0.63630974]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99269911]\n",
      " [0.97056638]\n",
      " [0.99831601]\n",
      " [1.30651518]]\n",
      "cost after\n",
      " [[0.38406181]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49494248]\n",
      " [0.69012844]\n",
      " [0.80466203]\n",
      " [0.46760603]\n",
      " [0.63634721]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99401389]\n",
      " [0.97159998]\n",
      " [0.99835129]\n",
      " [1.30773943]]\n",
      "cost after\n",
      " [[0.38412318]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49489383]\n",
      " [0.69022597]\n",
      " [0.80475415]\n",
      " [0.46751145]\n",
      " [0.63638239]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99532996]\n",
      " [0.97263498]\n",
      " [0.99838573]\n",
      " [1.30895889]]\n",
      "cost after\n",
      " [[0.38418407]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49484324]\n",
      " [0.69032126]\n",
      " [0.80484448]\n",
      " [0.46741511]\n",
      " [0.63641545]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99664718]\n",
      " [0.97367123]\n",
      " [0.99841932]\n",
      " [1.31017387]]\n",
      "cost after\n",
      " [[0.38424451]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49479089]\n",
      " [0.69041449]\n",
      " [0.80493315]\n",
      " [0.46731715]\n",
      " [0.63644656]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99796541]\n",
      " [0.9747086 ]\n",
      " [0.99845206]\n",
      " [1.31138466]]\n",
      "cost after\n",
      " [[0.38430454]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49473689]\n",
      " [0.69050581]\n",
      " [0.80502027]\n",
      " [0.46721769]\n",
      " [0.63647586]]\n",
      "###########################################\n",
      "grad after\n",
      " [[0.99928452]\n",
      " [0.97574695]\n",
      " [0.99848394]\n",
      " [1.31259153]]\n",
      "cost after\n",
      " [[0.38436418]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4946814 ]\n",
      " [0.69059537]\n",
      " [0.80510595]\n",
      " [0.46711685]\n",
      " [0.63650348]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00060439]\n",
      " [0.97678616]\n",
      " [0.99851494]\n",
      " [1.31379473]]\n",
      "cost after\n",
      " [[0.38442345]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49462452]\n",
      " [0.69068329]\n",
      " [0.8051903 ]\n",
      " [0.46701476]\n",
      " [0.63652956]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00192491]\n",
      " [0.97782612]\n",
      " [0.99854507]\n",
      " [1.31499447]]\n",
      "cost after\n",
      " [[0.38448239]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49456636]\n",
      " [0.6907697 ]\n",
      " [0.80527341]\n",
      " [0.4669115 ]\n",
      " [0.6365542 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00324599]\n",
      " [0.97886673]\n",
      " [0.99857431]\n",
      " [1.31619097]]\n",
      "cost after\n",
      " [[0.38454102]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49450704]\n",
      " [0.6908547 ]\n",
      " [0.80535535]\n",
      " [0.46680718]\n",
      " [0.63657752]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00456753]\n",
      " [0.9799079 ]\n",
      " [0.99860266]\n",
      " [1.31738443]]\n",
      "cost after\n",
      " [[0.38459935]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49444664]\n",
      " [0.69093841]\n",
      " [0.80543622]\n",
      " [0.46670188]\n",
      " [0.63659962]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00588944]\n",
      " [0.98094953]\n",
      " [0.99863012]\n",
      " [1.31857503]]\n",
      "cost after\n",
      " [[0.38465741]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49438526]\n",
      " [0.69102091]\n",
      " [0.80551607]\n",
      " [0.46659569]\n",
      " [0.63662058]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00721165]\n",
      " [0.98199154]\n",
      " [0.99865667]\n",
      " [1.31976293]]\n",
      "cost after\n",
      " [[0.38471522]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49432297]\n",
      " [0.6911023 ]\n",
      " [0.80559499]\n",
      " [0.46648867]\n",
      " [0.6366405 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00853407]\n",
      " [0.98303387]\n",
      " [0.99868232]\n",
      " [1.32094829]]\n",
      "cost after\n",
      " [[0.38477279]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49425985]\n",
      " [0.69118266]\n",
      " [0.80567303]\n",
      " [0.4663809 ]\n",
      " [0.63665944]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.00985666]\n",
      " [0.98407645]\n",
      " [0.99870707]\n",
      " [1.32213124]]\n",
      "cost after\n",
      " [[0.38483013]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49419597]\n",
      " [0.69126206]\n",
      " [0.80575025]\n",
      " [0.46627244]\n",
      " [0.63667749]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01117933]\n",
      " [0.9851192 ]\n",
      " [0.9987309 ]\n",
      " [1.32331193]]\n",
      "cost after\n",
      " [[0.38488727]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49413139]\n",
      " [0.69134058]\n",
      " [0.8058267 ]\n",
      " [0.46616335]\n",
      " [0.63669471]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01250203]\n",
      " [0.98616207]\n",
      " [0.99875381]\n",
      " [1.32449047]]\n",
      "cost after\n",
      " [[0.38494421]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49406617]\n",
      " [0.69141828]\n",
      " [0.80590244]\n",
      " [0.46605369]\n",
      " [0.63671117]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01382472]\n",
      " [0.987205  ]\n",
      " [0.99877581]\n",
      " [1.32566698]]\n",
      "cost after\n",
      " [[0.38500098]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49400038]\n",
      " [0.69149521]\n",
      " [0.8059775 ]\n",
      " [0.4659435 ]\n",
      " [0.63672691]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01514733]\n",
      " [0.98824795]\n",
      " [0.99879689]\n",
      " [1.32684156]]\n",
      "cost after\n",
      " [[0.38505756]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49393405]\n",
      " [0.69157144]\n",
      " [0.80605194]\n",
      " [0.46583284]\n",
      " [0.636742  ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01646983]\n",
      " [0.98929087]\n",
      " [0.99881705]\n",
      " [1.32801431]]\n",
      "cost after\n",
      " [[0.38511399]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49386723]\n",
      " [0.69164701]\n",
      " [0.80612578]\n",
      " [0.46572174]\n",
      " [0.63675648]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01779217]\n",
      " [0.99033372]\n",
      " [0.99883629]\n",
      " [1.32918532]]\n",
      "cost after\n",
      " [[0.38517027]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49379998]\n",
      " [0.69172198]\n",
      " [0.80619907]\n",
      " [0.46561025]\n",
      " [0.6367704 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.01911431]\n",
      " [0.99137645]\n",
      " [0.9988546 ]\n",
      " [1.33035466]]\n",
      "cost after\n",
      " [[0.3852264]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49373232]\n",
      " [0.69179638]\n",
      " [0.80627184]\n",
      " [0.4654984 ]\n",
      " [0.6367838 ]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02043622]\n",
      " [0.99241903]\n",
      " [0.99887198]\n",
      " [1.33152242]]\n",
      "cost after\n",
      " [[0.3852824]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4936643 ]\n",
      " [0.69187026]\n",
      " [0.80634412]\n",
      " [0.46538623]\n",
      " [0.63679672]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02175786]\n",
      " [0.99346143]\n",
      " [0.99888844]\n",
      " [1.33268867]]\n",
      "cost after\n",
      " [[0.38533827]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49359595]\n",
      " [0.69194365]\n",
      " [0.80641593]\n",
      " [0.46527377]\n",
      " [0.63680919]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02307921]\n",
      " [0.99450361]\n",
      " [0.99890396]\n",
      " [1.33385347]]\n",
      "cost after\n",
      " [[0.38539402]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.4935273 ]\n",
      " [0.6920166 ]\n",
      " [0.80648732]\n",
      " [0.46516105]\n",
      " [0.63682125]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02440024]\n",
      " [0.99554555]\n",
      " [0.99891856]\n",
      " [1.33501689]]\n",
      "cost after\n",
      " [[0.38544966]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49345838]\n",
      " [0.69208912]\n",
      " [0.80655829]\n",
      " [0.46504809]\n",
      " [0.63683293]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02572091]\n",
      " [0.99658722]\n",
      " [0.99893223]\n",
      " [1.33617898]]\n",
      "cost after\n",
      " [[0.38550519]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49338922]\n",
      " [0.69216125]\n",
      " [0.80662887]\n",
      " [0.46493493]\n",
      " [0.63684426]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.0270412 ]\n",
      " [0.99762859]\n",
      " [0.99894497]\n",
      " [1.33733978]]\n",
      "cost after\n",
      " [[0.38556062]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49331985]\n",
      " [0.69223302]\n",
      " [0.80669908]\n",
      " [0.46482157]\n",
      " [0.63685526]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.0283611 ]\n",
      " [0.99866965]\n",
      " [0.99895677]\n",
      " [1.33849936]]\n",
      "cost after\n",
      " [[0.38561596]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49325028]\n",
      " [0.69230445]\n",
      " [0.80676895]\n",
      " [0.46470805]\n",
      " [0.63686595]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.02968058]\n",
      " [0.99971036]\n",
      " [0.99896765]\n",
      " [1.33965775]]\n",
      "cost after\n",
      " [[0.3856712]]\n",
      "#############################################\n",
      "divide cost_erro by 2*m \n",
      " [[0.49318054]\n",
      " [0.69237557]\n",
      " [0.80683848]\n",
      " [0.46459439]\n",
      " [0.63687637]]\n",
      "###########################################\n",
      "grad after\n",
      " [[1.03099963]\n",
      " [1.00075072]\n",
      " [0.99897759]\n",
      " [1.340815  ]]\n",
      "cost after\n",
      " [[0.38572635]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    '''\n",
    "        we use vectorize version \n",
    "    '''\n",
    "    thetas = gradientdescent(X, thetas, y, X.shape[0], .09)\n",
    "    costs = costfunction(X,thetas,y,X.shape[0])\n",
    "    print(\"###########################################\")\n",
    "    print(\"grad after\\n\", thetas)\n",
    "    print(\"cost after\\n\",costs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03099963],\n",
       "       [1.00075072],\n",
       "       [0.99897759],\n",
       "       [1.340815  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79573801, -0.79573801, -0.23209025,  1.49200876],\n",
       "       [-0.79573801, -0.69627076, -0.0663115 ,  1.69094326],\n",
       "       [-0.79573801, -0.79573801,  0.        ,  1.79041051]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([\n",
    "    [1, 1, 18, 70],\n",
    "    [1, 4, 23,  76],\n",
    "    [1,1, 25, 79]\n",
    "])\n",
    "test = (test - np.mean(test))/ np.std(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53790558 0.66458614 0.68651322]]\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "d = 1/(1+(np.exp(-(np.dot(thetas.T, test.T)))))\n",
    "print(d)\n",
    "d = (d>=.5)+0\n",
    "print((d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
