{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e9750a",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are one of the powerful machine learning model, it based on the *Random Forset* and you can apply it to classifcation and regression problem.\n",
    "\n",
    "**Let take  look at one of these trees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39483b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "# !pip install graphviz\n",
    "from graphviz import Source\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039f1289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # just 2 features petal length and width \n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2) # the depth of the tree \n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944be138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(iris.feature_names[2:])\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54baf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using export_graphviz we can vislize this trained tree\n",
    "# dot file can be converted later to image or pdf using terminal\n",
    "f = open(\"/media/abdelrahman/SSD1/Free Work & internship/Desktop/Handson_ml/handson-ml-2/chapter_6/iris_tree.dot\", '+a')\n",
    "export_graphviz(\n",
    "        tree_clf, # The tree model\n",
    "        out_file=f,\n",
    "        feature_names=iris.feature_names[2:], # as we use just two features ( petal length and width )\n",
    "        class_names=iris.target_names,        # The classes we have in our data\n",
    "        rounded=True, \n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38459402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/abdelrahman/SSD1/Free Work & internship/Desktop/Handson_ml/handson-ml-2/chapter_6/iris_tree.dot.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_path = \"/media/abdelrahman/SSD1/Free Work & internship/Desktop/Handson_ml/handson-ml-2/chapter_6/iris_tree.dot\"\n",
    "output = Source.from_file(dot_path, format = \"png\") # can change png to pdf\n",
    "output.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b8f32",
   "metadata": {},
   "source": [
    "<img src=\"images/iris_tree.dot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7890a1b",
   "metadata": {},
   "source": [
    "# If condition\n",
    "\n",
    "so its all like if condition, you first check the **petal length**, based on the condition you go in the left or right side, once it true you have a leaf node tells you that you classify this as **sotosa**, if not you going to the right side then make another question and go in the tree depth, notic that you end up at depth 2 as you mentioned **max_depth=2**.\n",
    "\n",
    "- sampels attibute: counts  how  many  training  instances  it  applies  to.\n",
    "- value attribute: tells  you  how  many  training  instances  of  each  class  this  node applies to.\n",
    "- gini:gini  attribute  measures  its  impurity\n",
    "a node is “pure” (gini=0) if all training instances it applies to belong to the same class.  For  example,  since  the  depth-1  left  node  applies  only  to  Iris-Setosa  training instances, it is pure and its gini score is 0. \n",
    "\n",
    "<img src=\"images/1.png\">\n",
    "shows how the training algorithm  computes  the  gini  score.\n",
    "\n",
    "for example: node depth 2 left is 0.168 because,\n",
    "<img src=\"images/2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d1b63",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities\n",
    "\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class k: first it traverses the tree to find the leaf node for this instance, and then it returns  the  ratio  of  training  instances of  class  k  in  this  node, **note that its the ratio of training instances** means that all instance of some class k in particular node will took the same ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de7a8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f86113",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "first **petal_length > 2.45** so it goes to left node, then check for **petal_width <= 1.75** so it goes to left node in depth 2 and it leaf node, then:\n",
    "- first class is 0 because there are a 0 instance from 54 \n",
    "- second class is 49 so 49/54 = 0.90\n",
    "- third class is 5/54 = .09\n",
    "\n",
    "But of course  if  you  ask  it  to  predict  the  class,  it should output Iris-Versicolor (class 1) since it has the highest probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55ed741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e562758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[6, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe7761",
   "metadata": {},
   "source": [
    "# The CART Training Algorithm\n",
    "\n",
    "This is the algorith that the tree used to make the training, it first split the data into 2-subset based on single feature k and threhold t_k for that features like **petal length ≤ 2.45 cm**.\n",
    "How does it choose k and tk? It searches for the pair (k, tk) that produces the purest subsets (weighted by their size).\n",
    "\n",
    "it doing the same process as we dive in the tree untill reach the max_depth, or there is no split will reduce the impurity, and we can control this stopping based on other parameters like (min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29413b4a",
   "metadata": {},
   "source": [
    "# Computational Complexity\n",
    "\n",
    "We can see that it makes a prediction in O(log2(m)), as it traverse the tree  from  the  root  to  a  leaf. and ask just each time for one feature.\n",
    "\n",
    "But in training it cost O(n × m log(m)) because it compares all features (or max number of features if you set the paramter max_features) for all instances in each node to know the best pair (k, t_k) that produces the purest subsets.\n",
    "\n",
    "- n for all features\n",
    "- m for all instances \n",
    "- log(m) number of nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ed2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a92c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1401776580482603"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(49/54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75836bd",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n",
    "\n",
    "There are number of other hyperparameters that control the Decision tree from freedom of overfitting the data very will.\n",
    "\n",
    "-  min_samples_split  (the  minimum  number  of  samples a node must have before it can be split)\n",
    "-  min_samples_leaf (the minimum number of samples a leaf node must have)\n",
    "-  max_leaf_nodes  (maximum  number  of  leaf  nodes)\n",
    "-  max_features (maximum number of features that are evaluated for splitting at each node). \n",
    "- and other params that you can use\n",
    "\n",
    "The control pf these hyperparamters is must to reduce the model of overfitting for ex: the max_depth default value is None so it will split untill it overfitting the data very will.\n",
    "\n",
    "<img src=\"images/3.png\">\n",
    "\n",
    "On the left, the Decision Tree is trained with the default hyperparameters (i.e.,  no  restrictions),  and  on  the  right  the  Decision  Tree  is  trained  with  min_samples_leaf=4.  It  is  quite  obvious  that  the  model  on  the  left  is  overfitting,  and  the model on the right will probably generalize better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964499f9",
   "metadata": {},
   "source": [
    "# Regression Trees\n",
    "\n",
    "DecisionTreeRegressor is same as classification but predict value instead of class and this value will be the same for all instances in this node.\n",
    "\n",
    "# Instability \n",
    "\n",
    "The  main  issue  with  Decision  Trees  is  that  they  are  very  sensitive  to small variations in the training data and you  may get very different models even on the same training data (unless use the seed).\n",
    "\n",
    "But **Random Forests** can limit this instability by averaging predictions over many trees, as we will see in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbd2f4",
   "metadata": {},
   "source": [
    "# Solved exercise\n",
    "\n",
    "**Q1- What  is  the  approximate  depth  of  a  Decision  Tree  trained  (without  restrictions) on a training set with 1 million instances?**\n",
    "\n",
    "The complixity of Binary tree (as the case of all sklearn trees) is O(n*m*log2(m)).\n",
    "- n the number of features\n",
    "- m the number of instances\n",
    "\n",
    "Sklearn run a binary tree in all of its tree model, so let's try to understand from this point.\n",
    "\n",
    "A binary decision tree without restrictions, maybe end with one instance per leaf node, and the depth of the tree in this case will be log2(1000,000) wich around 20 as max_depth that the tree can reach based on number of instances in the data. \n",
    "\n",
    "This is simple example.\n",
    "\n",
    "<img src=\"images/d1.png\">\n",
    "\n",
    "**Q2- . Is a node’s Gini impurity generally lower or greater than its parent’s? Is it generally lower/greater, or always lower/greater?**\n",
    "\n",
    "Gini impurity generally lower than its parent’s, but it maybe not just in that node, but over all in that level will be lower than its parent’s, lets see with exampel:\n",
    "\n",
    "\n",
    "- for AAAAB First impurity = 1 - (⅕)**2 - (⅘)**2 = .32\n",
    "\n",
    "**Then it will split to two nodes maybe AAA and AB**\n",
    "\n",
    "- for AB Second impurity = 1 - (½)**2 - (½)**2 = .5 \n",
    "\n",
    "- for AAA Third impurity = 1 - (3/3)**2 = 0\n",
    "\n",
    "Over all error = ⅖ *.5 + ⅗ * 0 = .2 which less than the parent ^^\n",
    "\n",
    "**Q3- If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?**\n",
    "\n",
    "decreasing max_depth decrease the overfitting its fact, because it will decrease the number of leaf nodes which constrain the model , compare it to Q1 solution.\n",
    "\n",
    "**Q4- If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?**\n",
    "\n",
    "Features scaling with decision tree is waste of time because it does not care about the data is scaled or not, it made its decision like if condition.\n",
    "\n",
    "**Q5- . If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?**\n",
    "\n",
    "Time complixity of Deiciosn tree is = O(n*m*log2(m)) so:\n",
    "\n",
    "- For 1 million instances = O(n*1m*log2(1m)) = 1 h\n",
    "- for 10 million instances = O(n*10m*log2(10m)) \n",
    "\n",
    "let divide  O(n*10m*log2(10m))  / O(n*1m*log2(1m))  = about 11.7, see code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3223b6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 * np.log2(10**7) / np.log2(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439c808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
