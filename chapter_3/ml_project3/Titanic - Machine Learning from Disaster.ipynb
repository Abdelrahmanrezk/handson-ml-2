{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main file funcitons\n",
    "from configs import *\n",
    "\n",
    "# Main manipulation functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import  Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import  BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "from sklearn.model_selection import  StratifiedKFold, GridSearchCV\n",
    "from sklearn.base import  clone\n",
    "from sklearn.model_selection import  cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import  confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data = load_data('train.csv')\n",
    "\n",
    "# Display first 5 rows\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info Method\n",
    "\n",
    "As we are just displaying the data but some things we can notice from this display like **Nan** values, some of the data are numbers, others are object string, to go forward of other information about the total memory that the data used, or the data type for each feature and which is it contains **Nan** values or not and other information about the data we can use the **info** method associated with the pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE !!\n",
    "\n",
    "So some of information we can take from this is that we have:\n",
    "- 5 integers, 5 objects, 2 float columns\n",
    "- we have 891 instance, but as we can see some features have **Nan**\n",
    "- The memory usage can fit easily \n",
    "\n",
    "# Overview\n",
    "\n",
    "From what we have got from the **info** method, it will be useful to make a pipeline for each of these data, like pipeline handle **numbers**, others for **category** and at the end, we need to combine these two pipelines together.\n",
    "\n",
    "# Describe\n",
    "\n",
    "Let's check first numbers data and think of these numbers how they affect the target variable, which is here is the **Survived**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES !!\n",
    "\n",
    "Describe method working with numeric data, it provide you with some of statics about the feautres(columns data) you have,it work only on the existing values so nan values are ignored, it help you in like that:\n",
    "\n",
    "- 25% of people in the dataset we have their age is smaller than 20 years.\n",
    "- it help you also to know which max of each features, which help you to know if you need feature scaling.\n",
    "\n",
    "As we can see the data have different forms, numbers, and categories, so what we need is to handle these things separately and in the end, we can combine them in one pipeline.\n",
    "\n",
    "# Numeric Data\n",
    "\n",
    "We will start with the numeric data, spend some time in the analysis of these numbers, fill or remove nan values, and other work that may include making these features represent data well.\n",
    "\n",
    "### Numeric\n",
    "\n",
    "For Numeric attributes, I have ignored Passenger Id, as it looks like an index for each instance and its order will affect the model even if it does not have any information to the model.\n",
    "\n",
    "### Categories\n",
    "\n",
    "For Categories attributes, I have ignored the Name of the passenger and the ticket it holds, because it does not represent actual information also to the model, and use it is just hurts our process.\n",
    "\n",
    "### Target\n",
    "\n",
    "Also, I have one list for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get names of the attributes related to each pipeline we need to handle \n",
    "num_attr_names   = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass'] # reOrder attributes\n",
    "target_attr_name = ['Survived']\n",
    "cat_attr_names   = ['Sex', 'Embarked']\n",
    "tr_cols = num_attr_names + cat_attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leave the original data untouched\n",
    "titanic_data_copy = titanic_data.copy()\n",
    "titanic_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "As we know that our analysis of the data should be compiled on part of the dataset which is the training set, then once we have done the work we use the test set to ensure that the model can work in real life or not.\n",
    "\n",
    "The split we need is to ensure that the data split into training and testing in a way that includes each stratum (group) of points in each part of this splitting with the approximate ratio for each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    608\n",
       "1    209\n",
       "2     28\n",
       "4     18\n",
       "3     16\n",
       "8      7\n",
       "5      5\n",
       "Name: SibSp, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data_copy['SibSp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_copy['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_copy['Parch'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE !!\n",
    "\n",
    "As we can see some of the features have just 1 instance in the whole dataset so it will be in the training or in testing, so it may cause some miss learning as some data the model will see in testing and it may not see in the training or the opposite, also, split on these features may help but ignoring these outliers also can help so we have different trials we need to go forward.\n",
    "\n",
    "Trials like:\n",
    "\n",
    "- Try split based on Survived or SibSp or Parch or even two of these features.\n",
    "- Try to remove outliers (rare instances like in Parch class 6 is just come only 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how split working well for us\n",
    "stratified_tr_data, stratified_tes_data = split_data('Survived', titanic_data_copy)\n",
    "error_result = check_split_error(titanic_data_copy, stratified_tes_data, stratified_tes_data, 'Survived') \n",
    "error_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about Parch feautre !!\n",
    "stratified_tr_data, stratified_tes_data = split_data('SibSp', titanic_data_copy)\n",
    "error_result = check_split_error(titanic_data_copy, stratified_tes_data, stratified_tes_data, 'SibSp') \n",
    "error_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # What about Parch feautre !! Uncomment to see the error\n",
    "# stratified_tr_data, stratified_tes_data = split_data('Parch', titanic_data_copy)\n",
    "# error_result = check_split_error(titanic_data_copy, stratified_tes_data, stratified_tes_data, 'Parch') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE !!\n",
    "\n",
    "As we can see it give you an error because some of the classes has just 1 instance in the overall data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split is Done !! Let us Discover\n",
    "\n",
    "Now we have separated the dataset into train and test, then we need to go forward with some of the analysis to discover new insights about the data we dealing with, and as we mentioned above we will go first with Numeric Data.\n",
    "\n",
    "# Discover & Visualize Data to Gain Insights\n",
    "\n",
    "Just remember to keep the test set aside, and use just the training data, and if the training data is very large you can use a sample that represents overall the data because of the computation process on the machine.\n",
    "\n",
    "\n",
    "We know the target feature is the **Survived**, so what we will go on is to see this target variable with other features or even between the features itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr = stratified_tr_data.copy()\n",
    "titanic_tr_num = titanic_tr[num_attr_names]\n",
    "titanic_tr_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr_num.hist(bins=50, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Histograms\n",
    "\n",
    "We can see different shapes from normal distribution as **Age** to rights skewed in **Fare** and others just as numbers but it's like categorical data as it has just numbered a small number of class(discrete values).\n",
    "\n",
    "# Notes about Columns\n",
    "\n",
    "- parch: is children aboard the Titanic to this passenger\n",
    "- sibsp: is siblings aboard the Titanic to this passenger\n",
    "- fare: is the money that the passenger pay\n",
    "- cabin: Cabin number\n",
    "- embarked: from what place they have leave\n",
    "\n",
    "\n",
    "# Note !\n",
    "\n",
    "As most of the attributes like categorical attributes even of they are number but have a small number of discrete values, as well as the target variable is just 0 and 1.\n",
    "\n",
    "# Correlation\n",
    "\n",
    "Looking for how each attribute affects the target feature gives you some insights about the best attributes that describe the dataset very well as it helps a lot to predict the target, and what we have here is less correlation but we can not ignore these correlations like the positive correlation we have in **Fare** and the negative correlation with **Pclass**.\n",
    "\n",
    "It seems that we have also a correlation between **Fare** and **Pclass** and in the meaning of logic, it seems rights because the two related what this person is, maybe have more moeny so it a place take in first-class or something else and as we can see it negative correlation. ** Let's see it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = titanic_tr.corr()\n",
    "corr_matrix['Survived'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the mentioned above for Fare with Pclass\n",
    "corr_matrix['Fare'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['Age'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr_num.plot(kind='scatter', x='SibSp', y='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr_num.plot(kind='scatter', x='Pclass', y='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Attribute Combination\n",
    "\n",
    "We have to remove some features because it may miss the model to learn and it will affect our model, but on the other side add some features from the features we have may help the model so we need to test like this process, and we have the two ones **Parch** and **SibSp** can be added together to new attribute represent the overall of the family numbers.\n",
    "\n",
    "Also, the Negative correlation between **Age** and the two ones **SibSp** and **Pclass** can give us other insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot\n",
    "\n",
    "We can see some of the outliers in the scatter plot above as some instances are going away with Age, So we have to trying with and without like these outliers, or add regularization to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion from EDA ( Exploratory Data Analysis)\n",
    "\n",
    "## We have 12 column:\n",
    "- 3-int, 2-object, 2-float, 1-cat Target column, with 891 instances, some features contain NAN values.\n",
    "\n",
    "## Ignore some features like:\n",
    "- PassengerId Irrelevant feature\n",
    "- Name Irrelevant feature\n",
    "- Ticket, Irrelevant feature\n",
    "- Cabin, Because most of instances has Nan values (Poor Feature)\n",
    "- Try to remove also Embarked\n",
    "\n",
    "## Dealing with Nan Values:\n",
    "- For Age, Fare get mean\n",
    "- For Embarked, Parch, SibSp, Pclass, Sex   get mode (most frequent values)\n",
    "\n",
    "## Features scaling:\n",
    "- Age, Fare\n",
    "- Try also with and without Pclass, SibSp, Parch\n",
    "\n",
    "\n",
    "\n",
    "## Dealing with Outliers:\n",
    "- Parch feature\n",
    "- SibSp feature\n",
    "\n",
    "\n",
    "## Correlation:\n",
    "- Fare, Pclass most effect on target\n",
    "- Fare, Pclass have a medium negative correlation\n",
    "- Age, SibSp, Pclass also have a medium negative correlation\n",
    "\n",
    "## Combination:\n",
    "- Add Parch to SibSp to represent overall family.\n",
    "\n",
    "## OneHotEncoder\n",
    "\n",
    "We need to handle nominal categorical attributes like in **Sex**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Nan Values & outliers & Scaling features Pipeline\n",
    "\n",
    "# Outliers\n",
    "\n",
    "As we can see there are values that repeat once so trying with these outliers and without is a good assumption to go with, so I will try to remove the outliers class in attributes that repeated less or equal to only 10 times with the most repeated class like in case of **SibSp** replace class **5** with the class **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_pipelines(leave_remove=False, scaling=False):\n",
    "    num_imputer = ColumnTransformer([\n",
    "    ('mean_imputer', SimpleImputer(strategy='mean'), num_attr_names[:2]),\n",
    "    ('mode_imputer', SimpleImputer(strategy='most_frequent'), num_attr_names[2:]),\n",
    "    ])\n",
    "    num_pipeline = Pipeline([\n",
    "        ('outliers', LeaveOrRemoveOutLiers(['SibSp', 'Parch'], leave_remove)),\n",
    "        ('scaling', FeatureScale(num_attr_names, scaling)),\n",
    "        ('imputer', num_imputer),\n",
    "    ])\n",
    "\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "        ('num_pipeline', num_pipeline, num_attr_names),\n",
    "        ('cat_pipeline', OneHotEncoder(), cat_attr_names),\n",
    "    ])\n",
    "    return full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_ml(data, leave_remove=False, scaling=False):\n",
    "    # Replace nan values in categorical data separately before the pipeline\n",
    "    cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    cat_imputer = cat_imputer.fit_transform(data[cat_attr_names].values)\n",
    "    data[cat_attr_names] = cat_imputer # replace the imputed columns\n",
    "    \n",
    "    full_pipeline = initialize_pipelines(leave_remove, scaling)\n",
    "    data_prepared = full_pipeline.fit_transform(data)\n",
    "    return data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr_pipe_line = titanic_tr.copy()\n",
    "titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line)\n",
    "target_attr = np.array(titanic_tr[target_attr_name])\n",
    "titanic_tr_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination\n",
    "\n",
    "Just after the pipelined returned with the data add the combination you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# na_scl_oulrs_pili_trfm['OverallFamily'] = na_scl_oulrs_pili_trfm['SibSp'] + na_scl_oulrs_pili_trfm['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Select & Train a Model\n",
    "\n",
    "The problem we dealing with is  binary classification problem.\n",
    "\n",
    "The ways we deal with classification problems are different in the way of evaluating the model, trying to improve the result of cost function and others, this is because we looking for some discrete values just either 0 or 1 as in our problem **survived or not**.\n",
    "\n",
    "There are different Classification models one of them is the Stochastic Gradient Descent **SGD**, and this model is working very well with the large dataset as well as with multiple classification problems, but let's going on with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a copy of training data\n",
    "titanic_tr_pipe_line = titanic_tr.copy()\n",
    "titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line, False, False)\n",
    "titanic_tr_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data now is looking prepared lets print some instances\n",
    "titanic_tr_prepared[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take object from SGD model\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(titanic_tr_prepared, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets predict some labels and display the actuall ones\n",
    "test_3_instances = titanic_tr_prepared[:3]\n",
    "predict_3_instances = sgd_clf.predict(test_3_instances)\n",
    "print(\"The predicited labels: \\n\", predict_3_instances)\n",
    "# as we can see how the model predicts the first and third 1 correctly while it misses the second one.\n",
    "print(\"The actuall labels: :\\n\",  target_attr[:3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measurements\n",
    "\n",
    "Evaluating the classifier is strictly tricer than evaluating regressor, we have different measurements for the classifier model, so we will spend some time in this process.\n",
    "\n",
    "# Making accuracy using cross-validation\n",
    "\n",
    "- First using StratifiedKFold \n",
    "- Second using cross_val_score\n",
    "\n",
    "## using StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skfold.split(titanic_tr_prepared, target_attr):\n",
    "    clone_sgd = clone(sgd_clf)\n",
    "    \n",
    "    # Features\n",
    "    X_train_fold = titanic_tr_prepared[train_index]\n",
    "    X_test_fold  = titanic_tr_prepared[test_index]\n",
    "#     # Labels \n",
    "    y_train_fold = target_attr[train_index]\n",
    "    y_test_fold  = target_attr[test_index]\n",
    "    \n",
    "    clone_sgd.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = clone_sgd.predict(X_test_fold)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    m_correct = np.sum(y_pred == y_test_fold)\n",
    "    print(m_correct / len(y_test_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cross_val_score\n",
    "cross_val_score(sgd_clf, titanic_tr_prepared, target_attr, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note \n",
    "\n",
    "Even that the model gives a low score of 38% and in some fold relatively high score of 77%, but actually score not the preferred performance measurements, as it looks for those who classified correct for the positive class we search for which is here the **Survived people**, but also we need to know the result from other side related to those who are not **survived**, So.\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "the confusion matrix is a much better way to evaluate the performance of the classifier, it looks for the number of times instances of class A are classified as class B, in other words, the number of times the class **1** confused with the class **0**, here is the class **survived** confused with **not survived**.\n",
    "\n",
    "To compute the confusion matrix first we need the predicted class for each instance, so instead of using **cross_val_score** will use **cross_val_predict**, it return the resul of last fold.\n",
    "\n",
    "cross_val_predict combine the validate fold that test after training on each fold, then return one list contain the all div fold result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(sgd_clf, titanic_tr_prepared, target_attr, cv=3)\n",
    "confusion_matrix(pred, target_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in a confusion matrix represents an actual class, while each column represents a predicted class. The first row of this matrix considers 0(not survived) (the negative class): 264 of them were correctly classified as 0 (not survived) (they are called true negatives), while the remaining 100 were wrongly classified as 1 ((not survived)) (false positives). The second row considers (survived people) the (the positive class): 182 were wrongly classified as 0 (not survived) (false negatives), while the remaining 166 were correctly classified as 1 (survived people) (true positives). A perfect classifier would have only true positives and true negatives. so its confusion matrix would have nonzero values only on its main diagonal.\n",
    "\n",
    "\"The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric.**An interesting one to look at is the accuracy of the positive predictions (survived people)** this is called the precision of the classifier\", or looking for sensitivity (True Positive rate) which called **Recall**, the ratio of positive instances that are correctly detected by the classifier, even that they are not correctly classified as positive.\n",
    "\n",
    "Some times it's preferred to combine the precision and recall into a single matrix called f1_score, and this helps you to compare between two classifiers, The F1 score is the harmonic mean of precision and recall Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.\n",
    "\n",
    "# precision & Recall & F1-score\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/3.png\" /> </td>\n",
    "<td> <img src=\"images/2.png\" /> </td>\n",
    "\n",
    "<td> <img src=\"images/1.png\"/> </td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(pred, target_attr)) # its correct to detect 62% of survived people\n",
    "print(recall_score(pred, target_attr)) # it detects 47% of survived people.\n",
    "print(f1_score(pred, target_attr)) # only 54% is our performance of the model not as we see reach 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "In some application you preferred to have higher precision than recall or vise verse so it depends on the application you dealing with and **unfortunately, you can’t have it both ways: increasing precision reduces recall and vice versa. This is called the precision/recall trade-off**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall Trade-off\n",
    "\n",
    "\"To understand this trade-off, let’s look at how theSGDClassifier makes its classification decisions. For each instance, it computes a score based on a decision function, and if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class. Figure shows a few digits positioned from the lowest score on the left to the highest score on the right. Supposethe decision threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and one false positive (actually a 6). Therefore, withthat threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering thethreshold increases recall and reduces precision.\"\n",
    "\n",
    "<td> <img src=\"images/4.png\" /> </td>\n",
    "\n",
    "Scikit-Learn does not let you set the threshold directly, but you can use the decision function to know which score it used in prediction, and this can lead later as we will see to set the threshold to the value you need, to get high precision or low recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_instance = titanic_tr_prepared[0].reshape(1, -1)\n",
    "some_instance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = sgd_clf.decision_function(some_instance)\n",
    "threshold = 0\n",
    "y_instance_pred = (y_score > threshold)\n",
    "\n",
    "# make a correct prediction based on the threshold we set to\n",
    "print(y_score)\n",
    "print(target_attr[0])\n",
    "print(y_instance_pred)\n",
    "\n",
    "# What if we decrease the threshold \n",
    "\n",
    "threshold = -.7\n",
    "y_instance_pred = (y_score > threshold)\n",
    "# instead of false it have been True which is not a correct classifed\n",
    "print(y_score)\n",
    "print(target_attr[0])\n",
    "print(y_instance_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Note !\n",
    "\n",
    "This is good but its works with just one instance, but how we can decide which threshold to use? first, we can call predict but pass to it another argument which is decision_function, and it returns a clear prediction score for each instance, what means clear is that the prediction has made on a test set that the model never sees in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_score = cross_val_predict(sgd_clf, titanic_tr_prepared, target_attr.reshape(-1), cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have the score for all instances, so we are ready to get all precision and recall for all possible thresholds associated with these instances scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of argument is must\n",
    "precisions, recalls, thresholds = precision_recall_curve(target_attr, y_score)\n",
    "print(precisions[:3])\n",
    "print(recalls[:3])\n",
    "print(thresholds[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\") # -1 because threshold is less than by 1\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !!\n",
    "\n",
    "We can see that how the recall is going down when the threshold is increasing, on the vise vers the precision sometimes goes up down when we increase the threshold, but at the end, as we can see it go up increasing the threshold. and this point we can get from the graph in section **Precision/Recall Trade-off**, what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5(80%) down to 3/4 (75%), but at the end its go to 100% precisions. On the other hand, recall can only go down when the threshold is increased.\n",
    "\n",
    "# Plot precision_vs_recall dierctly\n",
    "\n",
    "Another way to select a good precision/recall tradeoff is to plot precision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recalls, precisions, \"r--\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision Vs Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !!\n",
    "\n",
    "If we are in case of interesting of persons, we can get precisions about 72% because its go down when recall is about 20% or less than 15%, on the other hand, the recall here will be about 15% which small recall but high precisions as we need,  but if we need the trade-off we can choose about 40% recall, which about 50% precision, it all depends on the project you are in.\n",
    "\n",
    "**Suppose that we looking for 72% precisions, how we can get the threshold.** ?\n",
    "\n",
    "From the graph previous of that, it shows you need a value greater than 5 thresholds, but we can search for the lowest threshold that gives at least 72% precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_72_precision = thresholds[np.argmax(precisions >= .72 )]\n",
    "y_pred_72_precision = (y_score >= threshold_72_precision)\n",
    "\n",
    "print(precision_score(target_attr, y_pred_72_precision))\n",
    "print(recall_score(target_attr, y_pred_72_precision))\n",
    "print(f1_score(target_attr, y_pred_72_precision))\n",
    "\n",
    "threshold_72_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !!\n",
    "\n",
    "Actually, we can get the precision value as we need, just set a high enough threshold, but high precision as we see above is low recall, and by default, the f1_score will be low, and for the application, we work with we need to see the result in fairly way as we need the actual result of who are survived and who are not.\n",
    "\n",
    "Also, another application may be you need high precision, like suggest videos to children, that avoid the sexual content for the child, so you have to skip some good videos low recall, **True Positive**, but keep only safe ones high precisions, so it depends on the project you dealing with.\n",
    "\n",
    "So **If someone says “let’s reach 99% precision,” you should ask, “at what recall?”**\n",
    "\n",
    "# The ROC Curve\n",
    "\n",
    "Thereceiver operating characteristic (ROC) curve is another common tool used with binary classifiers.It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROCcurve plots the true positive rate(another name for recall) against the false positive rate. The FPR is theratio of negative instances that are incorrectly classified as positive. It is equal to one minus the truenegative rate, which is the ratio of negative instances that are correctly classified as negative. The TNRis also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 – specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(target_attr, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr,tpr):\n",
    "    plt.plot(fpr, tpr) \n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Psoitive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall or Sensitivity)\")\n",
    "    plt.axis([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "As we can see the higher recall(TPR) means more False Positive instances that the classifer produce.\n",
    "\n",
    "Also, we can see that a good classifier is that one besides the top left(corner) of the graph as it possible, the dashed line shows that is bad random classifier if you got.\n",
    "\n",
    "Also, We can assume that about 20% recall is something good for low FPR, but in another application, we get high recall as we need also, but it will affect the precision as we mentioned they have **Inverse relationship **.\n",
    "\n",
    "\n",
    "\"Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and otherwise, use the ROC curve.\"\n",
    "\n",
    "# ROC AUC\n",
    "\n",
    "**\"One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target_attr, y_score) # seems not good as it very far away from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model\n",
    "\n",
    "As we can see it seems that the SGD model has not a good fit for the data it's underfitting the data, so we need to compare it to another model and see how this one will fit the data, we can use RandomForestClassifer, and its associated probability as in SGD descision_function to compare the ROC and ROC AUC of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foresr_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(foresr_clf, titanic_tr_prepared, target_attr.reshape(-1),\n",
    "                                    cv=3, method=\"predict_proba\")\n",
    "\n",
    "# it returns 2-scores of positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_score_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_fores, thresholds_forest = roc_curve(target_attr, y_score_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\") \n",
    "plt.plot(fpr_forest, tpr_fores, label=\"RFC\") \n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Psoitive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall or Sensitivity)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "As we can see that RFC is looking better and superior to the SGD classifier because its ROC curve is much closer to the top-left corner and actually it will has a greater AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target_attr, y_score_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest to get precision recall and f1\n",
    "y_pred_forest = cross_val_predict(foresr_clf, titanic_tr_prepared, target_attr.reshape(-1), cv=3)\n",
    "print(precision_score(target_attr, y_pred_forest))\n",
    "print(recall_score(target_attr, y_pred_forest))\n",
    "print(f1_score(target_attr, y_pred_forest))\n",
    "\n",
    "# Also we can deal with to increase the precision as we did with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knc_clf = KNeighborsClassifier()\n",
    "y_probas_knc = cross_val_predict(knc_clf, titanic_tr_prepared, target_attr.reshape(-1),\n",
    "                                    cv=3, method=\"predict_proba\")\n",
    "\n",
    "y_score_knc = y_probas_knc[:, 1] # score = proba of positive class\n",
    "fpr_knc, tpr_knc, thresholds_knc = roc_curve(target_attr, y_score_knc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_forest, tpr_fores, \"r:\", label=\"RFC\") \n",
    "plt.plot(fpr_knc, tpr_knc, label=\"KNC\") \n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Psoitive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall or Sensitivity)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target_attr, y_score_knc) # seems that close to RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune the Model\n",
    "\n",
    "We have the Random and K-ne are the best ones here so we can go forward to see who the best for us.\n",
    "\n",
    "# Note !!\n",
    "\n",
    "**Along with that, we will test the different pipelines we have discussed above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Take a copy of training data\n",
    "titanic_tr_pipe_line = titanic_tr.copy()\n",
    "titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line, False, False)\n",
    "print(titanic_tr_prepared.shape)\n",
    "grd_search = GridSearchCV(foresr_clf, param_grid, cv=3)\n",
    "grd_search.fit(titanic_tr_prepared, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grd_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_RandomForestClassifier(scale = False, leave = False):\n",
    "    \n",
    "    titanic_tr_pipe_line = titanic_tr.copy()\n",
    "    titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line, scale, leave)\n",
    "    print(titanic_tr_prepared.shape)\n",
    "\n",
    "    foresr_clf1 = RandomForestClassifier(max_features='auto',n_estimators=100, random_state=42)\n",
    "    predict = cross_val_predict(foresr_clf1, titanic_tr_prepared, target_attr.reshape(-1), cv=3)\n",
    "\n",
    "    print(precision_score(target_attr, predict))\n",
    "    print(recall_score(target_attr, predict))\n",
    "    print(f1_score(target_attr, predict))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RandomForestClassifier(False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RandomForestClassifier(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RandomForestClassifier(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RandomForestClassifier(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 9, 15, 29]}]\n",
    "\n",
    "\n",
    "# Take a copy of training data\n",
    "titanic_tr_pipe_line = titanic_tr.copy()\n",
    "titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line, False, False)\n",
    "print(titanic_tr_prepared.shape)\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=3, verbose=3)\n",
    "grid_search.fit(titanic_tr_prepared, target_attr.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_KNeighborsClassifier(scale = False, leave = False):\n",
    "    \n",
    "    titanic_tr_pipe_line = titanic_tr.copy()\n",
    "    titanic_tr_prepared = prepare_data_for_ml(titanic_tr_pipe_line, scale, leave)\n",
    "    print(titanic_tr_prepared.shape)\n",
    "\n",
    "    knn_clf1 = KNeighborsClassifier(n_neighbors=9,weights='uniform')\n",
    "    predict = cross_val_predict(knn_clf1, titanic_tr_prepared, target_attr.reshape(-1), cv=3)\n",
    "\n",
    "    print(precision_score(target_attr, predict))\n",
    "    print(recall_score(target_attr, predict))\n",
    "    print(f1_score(target_attr, predict))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KNeighborsClassifier(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KNeighborsClassifier(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_KNeighborsClassifier(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foresr_clf_lst_model = RandomForestClassifier(max_features='auto',n_estimators=100, random_state=42)\n",
    "foresr_clf_lst_model.fit(titanic_tr_prepared, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "titanic_data = load_data('test.csv')\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ts_pipe_line = titanic_data.copy()\n",
    "titanic_ts_prepared = prepare_data_for_ml(titanic_ts_pipe_line, True, False)\n",
    "predict = foresr_clf_lst_model.predict(titanic_ts_prepared)\n",
    "len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ts_pipe_line = titanic_tr.copy()\n",
    "titanic_ts_prepared = prepare_data_for_ml(titanic_ts_pipe_line, True, False)\n",
    "titanic_ts_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_svc = SVC(kernel='poly', degree=5)\n",
    "len_svc.fit(titanic_ts_prepared, target_attr)\n",
    "predict = len_svc.predict(titanic_ts_prepared)\n",
    "print(precision_score(target_attr, predict))\n",
    "print(recall_score(target_attr, predict))\n",
    "print(f1_score(target_attr, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ts_pipe_line = titanic_data.copy()\n",
    "titanic_ts_prepared = prepare_data_for_ml(titanic_ts_pipe_line, True, False)\n",
    "predict = len_svc.predict(titanic_ts_prepared)\n",
    "len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts_resul = pd.DataFrame({'PassengerId': titanic_data['PassengerId'], 'Survived': predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts_resul.to_csv('datasets/test_result_svc_5_01_poly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.read_csv('datasets/test_result_svc_5_01_poly.csv')\n",
    "test_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
